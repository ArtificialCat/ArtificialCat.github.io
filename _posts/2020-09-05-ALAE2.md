---
title:  "Adversarial Latent Auto-Encoder(ALAE) -(2)"
excerpt: "StyleALAE 모델 구조와 StyleGAN과의 비교, 인코딩된 latent w를 통한 manipulation을 확인해보자"

categories:
  - AI
tags:
  - StyleGAN
  - Adversarial Loss
  - Auto-encoder
  - Generator-Encoder Map
last_modified_at: 2020-09-05T22:30:00-05:00
use_math: true
---
# 4. StyleALAE
[지난 포스트](https://artificialcat.github.io/ai/ALAE/)에서는 기본적인 ALAE의 구조와 optimization 제약식들을 살펴보았다.
여기서는 Generator $G$를 StyleGAN을 활용하고 이 Generator의 구조에 맞는 대칭적인 Encoder구조를 붙임으로써 Encoder-Generator구조를 가지는 StyleALAE를 제시한다.
{: .text-left}
![StyleALAE 구조](/images/stylealae_structure.png)
{: .image-center}  
  
오른쪽의 Generator는 Nvidia에서 발표한 StyleGAN(v1)의 구조이다. 이 생성 모델에 대한 자세한 설명은 다른 StyleGAN 논문을 참조하기 바라며, 간단하게 컨셉만 잡고 넘어가면 이미지는 여러 레벨의 스타일 벡터의 합성이라는 컨셉으로 요약할 수 있다. 
     
해당 생성 모델에는 Style이 각 Block마다 들어가게 되므로, Encoder는 이와는 대칭적인 구조로 구성되었다.  
Encoder의 구조를 살펴보면 각 Generator에 각 Block당 2개의 style input이 들어가기 때문에 이 input을 생성하도록 마찬가지로 각 Encoder block에 Instance Normalization을 통해 각 channel의 instance average와 standard deviation를 뽑아낸다.  
$y_i^E$를 Encoder의 $i$번째 $layer$의 output이라고 한다면, $\mu(y_i^E)$와 $\sigma(y_i^E)$를 추출할 수 있으며 이 2개의 statistics을 통해 style 벡터를 표현할 수 있다. 해당 내용이 잘 이해되지 않는다면 [Adain](https://arxiv.org/pdf/1703.06868.pdf)을 참조하면 도움이 된다.  
이렇게 각 Encoder Block에서 추출된 Style 벡터를 G의 Adain의 input latent w로 만들기 위해서 multilinear-map을 통해 선형결합한다. $C_i$는 learnable parameter이다.  
{: .text-left}
$w = \sum_{i=1}^{N}C_i\begin{bmatrix} \mu(y_i^E) \cr \sigma(y_i^E) \end{bmatrix}$
{: .text-center}

이렇게 생성된 w를 기존 StyleGAN과 마찬가지로 4x4 ~ 1024x1024까지 각 Resolution Block에 Style 벡터가 들어가게 되고 각 Resolution Block에서 다음 Resolution Block까지 Progressive Growing 컨셉으로 이전 Layer의 합성 이미지와 다음 Layer의 합성이미지를 smoothly blending해서 낮은 해상도에서부터 높은 해상도까지 학습하게 된다. (v2에서는 이러한 Progressive한 컨셉이 사라지게 된다.)  
$F$, $D$는 모든 layer가 같은 Dimension을 가지는 MLP로 구성되었으며 F는 8개, D는 3개의 Layer로 구성하였다. Train algorithm은 아래 표와 같다.  
  
![Train ALAE](/images/train_alae.png)
{: .image-center}
학습 과정을 보면 총 3개의 step으로 구성되어 있다.  
  
>step 1. $E$, $D$의 parameter update로 $L_{adv}^{E,D}$는 앞에서 봤던 objective 함수 $V(G,D)$와 zero-centered gradient penalty term을 추가하였다. Discriminator 파트를 학습하게 된다.  
  
>step 2. $F$, $G$의 parameter update로 $L_{adv}^{F,G}$는 $softplus(-D \circ E \circ G \circ F)$로 Generator 파트를 학습하게 된다.  
  
>step 3. $E$, $G$의 parameter update로 $L_{error}^{E,G}$는 $latent$ $space$ $autoencoder$ $reciprocity$를 학습하게 된다.
  
# 5. Experiments
실험은 ALAE에 대해 MNIST, StyleALAE에 대해 FFHQ, Celeba, Bedroom 데이터 등으로 실험하였다.
다양한 실험 결과가 있으나 개인적으로 관심있었던 것은 StyleALAE에 관한 결과였으므로 해당 결과 위주로 확인해보았다.  
먼저 StyleGAN과의 PPL(Perceptual Path Length)을 비교해본 결과 latent space에서 StyleGAN보다 좋은 Disentanglement 성능을 볼 수 있었다.  
![PPL_FFHQ](/images/ppl_ffhq.png)
{: .image-center}
그렇다면 이미지 생성은 자연스럽게 될까? 결과는 다음과 같다.  
![Unseen_Reconst](/images/recon_unseen.png)
![Variety Resolution](/images/variety_resolution_reconstruction.png)
{: .image-center}
  
개인적으로는 생각보다는 괜찮았다. 실질적으로 unseen 이미지 복원에 이걸 사용할 목적이 아니기도 했고 Face 이미지의 특성에 따라 disentangle하게 w값이 잘 흩뿌려지게 encoder가 학습되기만 해도 만족했을텐데 $G \circ E(x)$ 값이 꽤나 뭉그러지지 않고 자연스럽게 비슷한 느낌의 얼굴을 생성해낸 것 같다.  
source set의 각 레벨의 style을 detination set의 이미지에 입힌 결과는 다음과 같다.  
![manipulation](/images/manipulation.png)
{: .image-center}
위에서부터 아래로 Coarse to Fine Feature를 다루고 있다. 위의 이미지를 보면 Coarse level에서는 헤어스타일, 포즈 등과 같은 특징은 source set으로부터 가져오고 다른 특성은 destination set의 특성을 따른다. 그에 반해 Fine level에서는 피부색이나 미세한 얼굴구조만 source set으로부터 가져오게 된다.  
  
이렇게 ALAE의 구조를 살펴볼 수 있었고, 이러한 Encoder-Generator 구조를 살펴보게 된 것은 기존의 pretrained GAN 모형에 unseen image를 latent space에 encode하는데 [(Puzer, StyleGAN-Encoder)](https://github.com/Puzer/stylegan-encoder)  2080Ti나 V100같은 GPU로 학습을 해도 이미지마다 lazy training을 해야하며 시간이 오래걸리는 문제가 발생하게 되서 였는데 한번 개인적으로 활용해야하는 도메인에 적용을 해볼만한 가치는 있는 것 같다.



